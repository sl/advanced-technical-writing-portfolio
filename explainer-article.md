# Learning From the Machine
#### By Sam Lazarus

![Deep Mind](https://cdn-images-1.medium.com/max/2600/0*NFjaeBpAtq01Urqz)

It’s January of 2019.

Dario “The Little One” Wünsch, know by the moniker TLO, waltzed into the headquarters of the DeepMind project with his head high, ready to play the game he had dedicated his life to mastering, StarCraft II. The professional gamer celebrity has spent the better part of the decade playing the game as his full time job. That’s well over 500 hours working to perfect everything from high level strategy, to moment to moment reactions and gameplay. His opponent however has committed itself even more to the game. In its relatively short life, it has played about two hundred years of StarCraft II. Yes, that’s right. Years.

For the first time ever, Wünsch is truly living up to his name, The Little One, by taking on an opponent who has played more years of Starcraft II than Wünsch has lived in his life. His opponent today is a machine created by Google’s DeepMind project called AlphaStar.

TLO had many reasons to be confident that day. Machine learning experts have had their eyes on Starcraft II for years, and any attempts to create bots that can play on a human level have been trounced by professional players. Recently, machine learning has made great strides, with the same DeepMind team Wünsch faced today creating bots that can play at a super human level in the game of go, which was once seen as another near impossible game for computers. Starcraft II, however, represents a unique challenge for computers that has made it an even more elusive game. In games like go, each player can see the board at all times making go a “perfect information game.” All players have all the information. In StarCraft II though, you can only see parts of the board where your units are currently standing making it an “imperfect information game.” Part of the skill of StarCraft II is determining what your opponent might be up to based on only the limited information you have about their actions. Winning such games has been seen as a holy grail of sorts for machine learning researchers, as past attempts to deal with ambiguity by the machine learning research community had all fallen far short of human levels. The odds that DeepMind would succeed seemed low.

For all his confidence in his ability, in his five game series against AlphaStar, The Little One *was unable to take a single game.*

If it was able to succeed where so many other projects had failed, something must clearly be special about AlphaStar. What technologies lead to the shocking victory this January? What allowed the little bot that could to defeat The Little One that apparently couldn’t?

![AlphaStar Game](https://cdn-images-1.medium.com/max/1600/0*k2Bcc9eDj_fwvD1x.png)

Pictured: Alphastar playing its debut match against The Little One

AlphaStar is an example of a relatively new technology in the world of computer science called adversarial deep reinforcement learning. Let’s break that term apart to get a sense for what all that means. Reinforcement learning, also called Q learning, was one of the first techniques devised in the field of machine learning. You create a simple program that tries random things at first, and is “rewarded” when it does something good, and “punished” when it does something bad. When it’s rewarded, it remembers what is currently happening, and in future, prioritizes doing the things it was attempting leading up to when it was rewarded. The closer in time to the reward, the more likely it becomes to do it again. When the agent is punished, it modifies its behavior in the opposite way, avoiding actions it attempted recently. The agent does this by looking at the world around it, and assigning a value to every possible action it can take. This is called a Q value. When it takes an action, and then is rewarded, it increases the Q value of that action in that situation and vice versa. After playing long enough, you can come up with Q values for every possible move. This technology is great for playing extremely simple games such Tic Tac Toe, where the agent can see every state the board can be in, and learn the best responses to them. It can start to fail as games get more complex though. Take chess for example. In chess, there are upwards of 10¹²⁰ possible ways a game can play out. Generating Q values for that many states would take playing more games than there are atoms in the universe meaning that technique isn’t feasible even for the fastest computers.

To solve this problem, we employ the “deep” part of adversarial deep reinforcement learning. Deep learning is a machine learning architecture that specializes in pattern recognition. It learns to predict results based on past results it’s seen. Using deep learning, you can train your agent to guess what Q values should be for a given board state. This suddenly allows you to play much more complex games than Tic Tac Toe. A problem still remains though. In order to get to human levels of play, a deep learning agent needs to play far more games than a human. While humans can start playing games with intuition based on their previous experiences, machine learning agents don’t have past experiences! They start playing games in a completely random manner. To actually play like a human then, they have to do a lot more work.

Humans can’t play enough games against these computers for them to start beating us, as that would literally require us to play two hundred years of StarCraft against a computer, so finally, we bring in the “adversarial” part of adversarial deep reinforcement learning. If we have the machine play different versions of itself, we don’t have to waste time playing against a human. Additionally, we’re no longer constrained by moving at human speed. AlphaStar can simulate a game of Starcraft II against itself in minutes, meaning it can learn incredibly fast. By pitting different versions of AlphaStar against each other, DeepMind was able to eventually create a version that surpassed human levels of play, and took down The Little One.

![Artosis Analysis](https://cdn-images-1.medium.com/max/2600/1*G58RXOL-elClhxSRWej-hg.png)

Pictured: Daniel Ray “Artosis” Setmkoski, a professional StarCraft II commentator, analyzing TLO’s games against AlphaStar.

So with this victory, and a victory over another professional player, MaNa, in the following days, machines have claimed yet another game over humans. But what makes this an interesting story is not the defeat, but what we were able to learn from it. It turns out one of the biggest advantages AlphaStar has over humans is that it doesn’t take anything as a given. Because it eschews tradition, AlphaStar was able to attempt strategies that most professional players would dismiss as silly, or suboptimal. Upon analysis though, many of the seemingly strange strategies AlphaStar employed turned out to have hard to perceive advantages over their alternatives. For example at the start of the game, AlphaStar created too many probes. Probes are a basic resource generation tool in StarCraft II. They allow you to purchase other units throughout the game. Most players would laugh at a player that created too many probes. What people didn’t realize until analyzing the games further though is that AlphaStar had correctly predicted the amount of probes his opponent would kill off early in the game, and was making extra ones to compensate. It turns out that AlphaStar came up with a better strategy than humans had, and in a way that could easily be implemented into the play of existing pro players.

MaNa noticed these patterns and began studying. He watched the replays of his games against AlphaStar, he watched analysis by other players and commentators such as Artosis pictured above, he absorbed as much information about his game against AlphaStar as he could. A few weeks after his initial match, he challenged AlphaStar again, this time live in front of a large audience. In this game, the first thing the commentators noticed was MaNa doing exactly what those same commentators had mocked AlphaStar for doing in its first games agains The Little One. MaNa made “too many” probes. Using AlphaStar’s own strategies, MaNa turned around the machine trumps human narrative defeating AlphaStar convincingly in his rematch. By learning from the machine, MaNa was able to continue to keep moving himself forward.

As machine learning makes its way into many of our lives, we should take after MaNa. Rather than fearing its ability, realize it’s important that we realize its potential to not only make our lives easier, but to give us insight into the way we think, and how it can be changed for the better.

### Citations

- DeepMind, Google. “DeepMind StarCraft II Demonstration.” YouTube, YouTube, 24 Jan. 2019, www.youtube.com/watch?v=cUTMhmVh1qs.
- DeepMind, Google. “AlphaStar: Mastering the Real-Time Strategy Game StarCraft II.” DeepMind, DeepMind, 24 Jan. 2019, deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/.
- Stemkoski, Daniel Ray. “AlphaStar — Analysis by Artosis.” YouTube, YouTube, 28 Jan. 2019, www.youtube.com/watch?v=_YWmU-E2WFc.